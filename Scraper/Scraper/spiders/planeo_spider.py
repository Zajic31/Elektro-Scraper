from scrapy.spiders import Spider
from urllib.parse import urljoin, urlparse, parse_qs, urlunparse
from scrapy.http import Request

class PlaneoSpider(Spider):
    name = "planeospider"
    allowed_domains = ["planeo.cz"]
    
    # Krok offsetu je 24, jako u Datartu
    OFFSET_STEP = 24

    # 游꿢 START: Tyto URL funguj칤 perfektn캩 a pokr칳vaj칤 hlavn칤 kategorie.
    start_urls = [
        "https://www.planeo.cz/velke-domaci-spotrebice",
        "https://www.planeo.cz/tv-foto-audio-video",
        "https://www.planeo.cz/mobily-a-chytre-hodinky",
        "https://www.planeo.cz/notebooky-pocitace-a-tablety",
        "https://www.planeo.cz/male-domaci-spotrebice",
        "https://www.planeo.cz/dilna-a-zahrada",
        "https://www.planeo.cz/herni-konzole-zabava-media",
        "https://www.planeo.cz/osvetleni",
        "https://www.planeo.cz/dum-a-domaci-potreby",
        "https://www.planeo.cz/vyhodne-sety",
        "https://www.planeo.cz/hracky",
        "https://www.planeo.cz/chytra-domacnost",
        "https://www.planeo.cz/sport-a-outdoor",
        "https://www.planeo.cz/multifunkcni-prislusenstvi",
        "https://www.planeo.cz/nabytek",
        "https://www.planeo.cz/auto-moto",
        "https://www.planeo.cz/hodinky-a-hodiny",
        "https://www.planeo.cz/hudebni-nastroje",
        "https://www.planeo.cz/kancelar-a-papirnictvi",
        "https://www.planeo.cz/potraviny",
        "https://www.planeo.cz/chovatelske-potreby",
        "https://www.planeo.cz/drogerie",
        "https://www.planeo.cz/dtest"
    ]

    def parse(self, response):
        # 1. Extrakce produkt콢 (LIST SCRAPING Z GTM ATRIBUT콡)
        product_tiles = response.css('.c-product--catalogue[data-gtm-product-id]')
        
        if not product_tiles:
            # D콢le쬴t칠: Pokud na str치nce nejsou produkty, ukon캜칤me str치nkov치n칤 pro tuto kategorii.
            self.logger.info(f"Str치nkov치n칤 dokon캜eno na URL: {response.url} (Nenalezeny 쮂멳n칠 produkty/dla쬯ice)")
            return
        
        # Extrakce dat (z콢st치v치 beze zm캩ny, je funk캜n칤)
        for tile in product_tiles:
            item_name = tile.attrib.get('data-gtm-product-name')
            item_price_gross = tile.attrib.get('data-gtm-product-price')
            item_category_raw = tile.attrib.get('data-gtm-product-item-category')
            item_brand = tile.attrib.get('data-gtm-product-brand')
            link_path = tile.css('a::attr(href)').get()
            
            item_link = urljoin(response.url, link_path) if link_path else None
            rating_value = tile.css('span[data-testid="catalogue.item.rating.value-rating"]::text').get()
            
            item_rating = float(rating_value.replace(',', '.')) if rating_value else None
            item_price = float(item_price_gross) if item_price_gross else None

            # --- V칗SLEDKOV칗 YIELD ---
            if item_name and item_price is not None and item_link:
                yield {
                    "title": item_name.strip(),
                    "price": item_price,
                    "link": item_link,
                    "rating": item_rating,
                    "category": item_category_raw
                }
                
        # 2. Navigace na dal코칤 str치nku (STR츼NKOV츼N칈 POMOC칈 INKREMENTACE OFFSETU)
        
        # a) Zjist칤me aktu치ln칤 URL
        current_url = response.url
        
        # b) Rozparsujeme URL, abychom zjistili aktu치ln칤 offset
        parsed_url = urlparse(current_url)
        query_params = parse_qs(parsed_url.query)
        
        # c) Zjist칤me aktu치ln칤 offset nebo nastav칤me na 0
        current_offset = 0
        if 'offset' in query_params:
            try:
                current_offset = int(query_params['offset'][0])
            except ValueError:
                current_offset = 0 # Pokud je offset vadn칳, za캜neme od nuly

        # d) Vypo캜칤t치me nov칳 offset
        new_offset = current_offset + self.OFFSET_STEP
        
        # e) Vytvo콏칤me novou URL s nov칳m offsetem
        query_params['offset'] = [str(new_offset)]
        
        # Znovu sestav칤me query string a celou URL
        new_query = "&".join([f"{k}={v[0]}" for k, v in query_params.items()])
        
        # Vytvo콏칤me novou URL (sch칠ma, netloc, path, params, query, fragment)
        next_page_url = urlunparse(parsed_url._replace(query=new_query))
        
        self.logger.info(f"Vytv치콏칤m odkaz na dal코칤 str치nku: {next_page_url}")
        
        # f) Pokra캜ujeme na novou URL
        # Pou쬴jeme Request, abychom se vyhnuli chyb치m v response.follow
        yield Request(url=next_page_url, callback=self.parse)